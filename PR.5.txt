Aim: Write a Spark code for the given application and handle error and recovery of data

sudo apt update

sudo apt install curl -y

sudo apt install gnupg -y

sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 \
 --recv-keys 2EE0EA64E40A89B84B2DF73499E82A75642AC823

echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | \
 sudo tee /etc/apt/sources.list.d/sbt.list

echo "deb https://repo.scala-sbt.org/scalasbt/debian /" | \
 sudo tee /etc/apt/sources.list.d/sbt_old.list

sudo apt update

sudo apt install sbt -y

echo "deb [signed-by=/usr/share/keyrings/sbt-archive-keyring.gpg] https://repo.scala-sbt.org/scalasbt/debian all main" | sudo tee /etc/apt/sources.list.d/sbt.list

curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | sudo apt-key add -

sudo apt-get update

sbt


#Install scala

wget https://downloads.lightbend.com/scala/2.12.18/scala-2.12.18.deb

sudo dpkg -i scala-2.12.18.deb

scala -version

nano ExceptionHandlingTest.scala

  add below code,
import org.apache.spark.sql.SparkSession
object ExceptionHandlingTest {
 def main(args: Array[String]): Unit = {
 val spark = SparkSession
 .builder()
 .appName("ExceptionHandlingTest")
 .master("local[*]") // Optional: helpful for local debugging
 .getOrCreate()
 val rdd = spark.sparkContext.parallelize(0 until spark.sparkContext.defaultParallelism)
 rdd.foreach { i =>
 try {
 if (math.random > 0.75) {
 throw new Exception("Testing exception handling")
 } else {
println(s"Task $i completed successfully")
 }
 } catch {
 case e: Exception =>
 println(s"Exception in task $i: ${e.getMessage}")
 }
 }
 spark.stop()
 }
}

$SPARK_HOME/sbin/start-master.sh

$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077

ls
mkdir ~/ExceptionHandlingProject
cd ~/ExceptionHandlingProject
mkdir -p src/main/scala
mv ~/ExceptionHandlingTest.scala src/main/scala/
nano build.sbt

add below code,
scalaVersion := "2.12.18"
libraryDependencies ++= Seq(
 "org.apache.spark" %% "spark-core" % "3.4.1",
 "org.apache.spark" %% "spark-sql" % "3.4.1"
)

sbt package

spark-submit \
 --class ExceptionHandlingTest \
 --master local[*] \
 target/scala-2.12/exceptionhandlingproject_2.12-0.1.0-SNAPSHOT.jar
